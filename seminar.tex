\documentclass[	runningheads,
				a4paper]{llncs}
\usepackage{url}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{hyperref}

% Support for special characters like "Umlaute"
\usepackage[utf8]{inputenc}


\usepackage[english]{babel}


%*********************************************************************%
% META                                                                %
%*********************************************************************%

\newcommand{\university}{Saarland University}
\newcommand{\school}{Saarland Informatics Campus}


\newcommand{\thetitle}{Seminar: Understanding of configurable software systems}
\newcommand{\shorttitle}{Seminar: Understanding of configurable software systems}
\newcommand{\thedate}{January 11}


\newcommand{\theforename}{Bipin}
\newcommand{\thesurname}{Oli}

% Advisors

\newcommand{\advisor}{Advisors}

\newcommand{\advisors}{Prof. Sven Apel, \\ Christian Hechtl}

% Title for the seminar

\newcommand{\theseminartitle}{Dynamic symbolic execution (concolic execution)}


%*********************************************************************%
% THE DOCUMENT                                                        %
%*********************************************************************%

\begin{document}
%*********************************************************************%
% TITLE                                                               %
%*********************************************************************%

% Arabic page numbering
\mainmatter 
	
% Title including a subtitle for the name of the seminar
\title{\theseminartitle \\ \small \thetitle}

\author{\theforename\ \thesurname \small \\ \ \\ \advisor : \ \advisors}

% (Optional) This will appear near the page number
\authorrunning{\shorttitle}

\institute{\school ,\\ \university}


\maketitle
%*********************************************************************%
% CONTENT                                                             %
%*********************************************************************%
% Introduction
\section{Abstract}
Configurable systems with many dials and knobs brings in a big testing challenge. In presence of many possible variants and configuration options it is very important to automate the testing as much as possible. Directed automatic random testing, popularly known as concolic execution is a primary way how it is done. Concolic execution  is a software verification technique that performs symbolic execution together with concrete input values. Concrete values are selected with the help of a constraint solver to guide a program flow in a particular direction. The selection of concrete values helps to scale the verification to a larger program as it makes the symbolic constraints smaller by selecting specific branches in the program. Compared to random execution, this allows us to guide the analysis in a direction likely to have bugs which makes this technique powerful. However, in doing so, we sacrifice the completeness of the analysis in favor of the depth of analysis. The sheer number of branches in a large program makes it difficult to perform a complete analysis, so we have to prioritize the branches likely to contribute to finding a bug. There have been many studies to deal with this path explosion problem. In this paper, I have categorically presented them.

\section{Introduction}
Software is an integral component in many aspects of modern life. Many critical systems are driven by software. Complex relationship between man and machine, and the variety of applications has produced complex software. Much of this complexity is inherent in the problem, however the act of implementation also brings in its own complexity. Furthermore, due to the dynamic nature of human needs the software is required to evolve and change all the time. It is expected to operate under various conditions and use cases. This has given rise to configurable softwares with lot of configurable options and variants. It is important to verify the correctness of software to use it with confidence but this complexity, pace of change and the size of software makes it challenging to do so.

- random testing
- symbolic testing
- concolic testing
- concolic fuzzing
- SAGE and other examples 
- limitations 

One way to test a software is by providing it with random inputs. It is a black-box testing method 



\section{Introduction}
The main idea of concolic testing is to execute the programsimultaneously  with  concrete  values  and  symbolic  values.When the program is executed, symbolic constraints alongthe executed path are collected in a formula calledpath con-dition.  Then, a branch is picked and negated from the pathcondition resulting in a new formula which is then fed to aconstraint solver to check for satisfiability.  If it is satisfiable,concrete test inputs are generated to follow the new feasiblepath.   If  it  is  unsatisfiable,  the  new  path  is  infeasible  andanother branch has to be picked to be negated.  This wayconcolic testing attempts to improve the poor code coverageof random testing.  A key characteristic of concolic testing isthat path conditions can be simplified using concrete valueswhenever the decidability of their symbolic constraints goesbeyond the capabilities of the underlying constraint solver.One major problem with concolic testing is that there arein general an exponential number of paths in the programto  explore,  resulting  in  the  so-calledpath-explosionprob-lem.  Recently, several methods have been proposed to at-tack this problem from various angles:  using heuristics fo-cused on branch coverage [3], function summaries [8], usingstatic/dynamic  program  analysis  [2]  and  so  on.   We  pro-pose a new method based oninterpolation, largely comple-mentary to existing approaches, that significantly mitigatespath-explosion by pruning a potentially exponential numberof paths that can be guaranteed to not encounter a bug.

\begin{itemize}
	\item What is it?
	\item Where did it start?
	\item How does it work?
	\item Give an example
	\item Why is it important? (give example of use in configurable systems)
	\item It's contributions
	\item It's limitations (give context to configurable system with many branches)
	One  of  the  biggest  challenges  in  concolic  testing  is  thatthere  are  often  too  many  branches  to  select  for  the  nextinput.  This is referred to as thepath explosionproblem [10,11, 3].  The number of paths in the execution tree increasesexponentially with the number of branches in the program.Visiting only the top twenty branches in the execution treein a breadth first search (BFS) order requires more than onemillion concolic runs (220).  However, programs usually havefar more than twenty branches,  for example,  an executionpath  ofgrep,  a  15K  line  of  code  program,  contains  morethan  8,000  branches.   Therefore,  exploring  all  paths  in  anexecution tree in a reasonable amount of time is not feasible.

	\item Example of the use of this technique in finding bugs in configurable system eg result of SAGE, EXE, etc.
\end{itemize}


\section{Body}

% ------------------------------
\subsection{2007: Performing dynamic test generation compositionally}
\cite[paper]{godefroid2007compositional}

\subsubsection{Gist:}
The general idea behind this new search algorithm is to perform dynamic  test generation compositionally, by adapting (dualizing) known techniques for interproce-dural static analysis to the context of automated dynamic test gener-ation.

\subsubsection{Methodlogy:}
new search algorithm called SMART which stands for duce a new algorithm, dubbed SMARTfor Systematic Modular Automated Random Testing, a more efficent search method then DART without compromising
completeness. It tests functions in isolation, collects testing results as function summaries ex-pressed using preconditions on function inputs and postconditionson function outputs, and then re-use those summaries when testinghigher-level functions.

A SMART search performs dynamic test generation composi-tionally,  using  function  summaries  as  defined  previously.  Thosesummaries  are  dynamically  computed  in  a  top-down  mannerthrough the call-flow graphGPofP. Starting from the top-levelfunction, one executes the program (initially on some random in-puts) until one  reaches  a first functionfwhose execution termi-nates on a return orhaltstatement. One then backtracks insidefas much as  possible  using DART, computing summaries  for thatfunction and each of those DART-triggered executions. When thissearch (backtracking)  infis over, one then resumes the originalexecution wherefwas called,  this time treatingfessentially asa black-box,  i.e., without analyzing it and re-using its previouslycomputed  summary  instead. 

\subsubsection{Result:}
SMART can perform dynamic test generation com-positionally without any reduction in program path coverage. Wealso show that, given a bound on the maximum number of feasiblepaths in individual program functions, the number of program exe-cutions explored by SMART is linear in that bound, while the num-ber of program executions explored by DART can be exponentialin that bound.
SMART = scalable DART

% ------------------------------

\subsection{2006: Software Partitioning for Effective Automated Unit Testing}
\cite{chakrabarti2006software}

\subsubsection{Gist:}
present an approach thatidentifies control and data inter-dependencies between soft-ware components using static program analysis, and dividesthe source code into units where highly-intertwined compo-nents are grouped together. Those units can then be testedin isolation using automated test generation techniques andtools,  such as dynamic software model checkers

\subsubsection{Methodlogy:}
group together functions or components that share interfaces of complexity higher than 
a particular threshold. Complexity is determined by the popularity and sharing of code. The idea is that 
if the function is very popular and is being called from a lot of places then it is likely that it is not closely liked to any component. 
And, the sharing of code meaning if two functions share many of the same functions then it is likely that the higher level operation they perform is close to each other.

\subsubsection{Configurability part:}
evaluated the effectiveness by applying the algorithm to the open source implementation of oSIP protocol (http://www.gnu.org/software/osip/osip.html) which is a telephony protocol for call establishment. 


\subsubsection{Result:}
showing  that  auto-matic  software  partitioning  can  significantly  increase  testcoverage without generating too many false alarms causedby unrealistic inputs being injected at interfaces betweenunits

% ------------------------------
\subsection{2007: Hybrid concolic testing (**)}

\subsubsection{Gist:}
an algorithm that in-terleaves random testing with concolic execution to obtainboth a deep and a wide exploration of program state space.Our algorithm generates test inputs automatically by inter-leaving  random  testing  until  saturation  with  bounded  ex-haustive  symbolic  exploration  of  program  points.   It  thuscombines the ability of random search to reachdeeppro-gram states quickly together with the ability of concolic test-ing to explore states in a neighborhood exhaustively. 

\subsubsection{Methodlogy:}
presenthybrid concolic testing, a simple algorithmthat  interleaves  the  application  of  random  tests  with  con-colic  testing  to  achieve  deep  and  wide  exploration  of  theprogram  state  space.   From  the  initial  program  state,  hy-brid  concolic  testing  starts  by  performing  random  testingto improve coverage.  When random testingsaturates, thatis,  does  not  produce  any  new  coverage  points  after  run-ning  some  predetermined  number  of  steps,  the  algorithmautomatically switches to concolic executionfrom the cur-rent program stateto perform an exhaustive bounded depthsearch for an uncovered coverage point.  As soon as one isfound,  the algorithm reverts back to concrete mode.   Theinterleaving of random testing and concolic execution thususes both the capacity of random testing to inexpensivelygenerate deep program states through long program execu-tions and the capability of concolic testing to exhaustivelyand symbolically search for new paths with a limited looka-head.

The interleaving of random and symbolic techniques isthe crucial insight that distinguishes hybrid concolic testingfrom a naive approach that simply runs random and con-colic tests in parallel on a program.  This is because manyprograms show behaviors where the program must reach aparticular statesand then follow a precise sequence of in-put events 'alpha' order to get to a required coverage point.It  is  often  easy  to  reachsusing  random  testing,  but  notthen to generate the precise sequence of events 'alpha'.  On theother hand, while it is usually easy for concolic testing togenerate 'sigma', concolic testing gets stuck in exploring a hugenumber of program paths before even reaching the states.

In the end, hybrid concolic testing has the same limita-tions of symbolic execution based test generation:  the dis-covery of uncovered points depends on the scalability andexpressiveness of the constraint solver, and the exhaustivesearch  for  uncovered  points  is  limited  by  the  number  ofpaths to be explored. Therefore, in general, hybrid concolictesting may not achieve 100 percent coverage, although it can im-prove random testing considerably. Further, the algorithmisnot a panacea for all software quality issues. While we pro-vide an automatic mechanism for test input generation, allthe other effort required in testing, for example, test oraclegeneration, assertion based verification, and mock environ-ment creation still have to be performed as with any othertest input generation algorithm.  Further, we look for codecoverage,  which may or may not be an indicator of codereliability.

\subsubsection{Configurability part:}
compare random,  concolic,  andhybrid concolic testing on the VIM text editor (150K linesof C code) and on an implementation of the red-black treedata structure. Our experiments indicate that for a fixed test-ing budget, hybrid concolic testing technique outperformsboth random and concolic in terms of branch coverage. of the state space exhaustively. In contrast, hybrid concolictesting switches to inexpensive random test-ing as soon as it identifiessomeuncovered point, relying onfast random testing to explore as much of the state space aspossible.  In this way, it avoids expensive constraint solv-ing to perform exhaustive search in some part of the statespace. Moreover, if random testing does not hit a new cov-erage point, it can take advantage of the locally exhaustivesearch provided by concolic testing to continue from a newcoverage point

% ------------------------------
\subsection{2008: Heuristics for Scalable Dynamic Test Generation}

\subsubsection{Gist:}
several  such  heuristic  search  strategies,  including  anovel strategy guided by the control flow graph of the programunder test. 

\subsubsection{Methodlogy:}
We  propose  a  search  strategy  that  is  guided  by  the  staticstructure  of  the  program  under  test,  namely  the  control  flowgraph  (CFG).  In  this  strategy,  we  choose  branches  to  negatefor  the  purpose  of  test  generation  based  on  their  distance  inthe CFG to currently uncovered branches. We experimentallyshow  that  this  greedy  approach  to  maximizing  the  branchcoverage helps to improve such coverage faster, and to achievegreater   final   coverage,   than   the   default   depth-first   searchstrategy of concolic testing.We further propose two random search strategies. While intraditional random testing a program is run on random inputs,these  two  strategies  test  a  program  along  random  executionpaths. The second attempts to sample uniformly from the spaceof possible program paths, while the third is a variant we havefound to be more effective in practice

have implemented these search strategies in CREST, anopen-source prototype test generation tool for C

\subsubsection{Configurability part:}
We have implemented these strategies in CREST, ouropen source concolic testing tool for C, and evaluated them on twowidely-used software tools, grep 2.2 (15K lines of code) and Vim5.7 (150K lines). On these benchmarks, the presented heuristicsachieve significantly greater branch coverage on the same testingbudget than concolic testing with a traditional depth-first searchstrategy.


% ------------------------------
\subsection{2009: Fitness-Guided Path Exploration in Dynamic Symbolic Execution}

\subsubsection{Gist:}
To address the space-explosion  issue  in  path  exploration,  we  propose  a  novelapproach called Fitnex, a search strategy that uses state-dependent fitness values (computed through a fitness func-tion) to guide path exploration.  The fitness function mea-sures how close an already discovered feasible path is toa  particular  test  target  (e.g.,  covering  a  not-yet-coveredbranch).   Our new fitness-guided search strategy is  inte-grated with other strategies that are effective for explorationproblems where the fitness heuristic fails.

\subsubsection{Methodlogy:}
The core of our approach is the Fitnex search strat-egy guided by fitness values computed with a fitness func-tion (Section 4.1).  To deal with program branches notamenable to a fitness function, our approach includes in-tegration of the Fitnex strategy with other search strategies(Section 4.2)

A fitness function (Section 4.1.1) gives a measurementon how close an explored path is to achieving a test tar-get (e.g., covering a not-yet-covered branch). We computea fitness value for each already explored path and priori-tize these known paths based on their fitness values (Sec-tion 4.1.2). We compute a fitness gain for each branch in theprogram under test and prioritize branching nodes based ontheir corresponding branches’ fitness gains (Section 4.1.3).During path exploration, we give higher priority to flippinga branching node with a better (higher) fitness gain in a pathwith a better (lower) fitness value (Section 4.1.4).

% ------------------------------
\subsection{2013: Boosting Concolic Testing via Interpolation}
\subsubsection{Gist:}
propose  a  new  and  complementarymethod based oninterpolation, that greatly mitigates path-explosion by subsuming paths that can be guaranteed to nothit a bug.

\subsubsection{Methodlogy:}
first,  assume  that  the  program  is  annotatedwith  certain  bug  conditions  of  the  form “if C then bug”,where  if  the  conditionCevaluates  to  true  along  a  path,the  path  is  buggy.   Then,  whenever  an  unsatisfiable  pathcondition  is  fed  to  the  solver,  an  interpolant  is  generatedat each program point along the path.  The interpolant at agiven program point can be seen as a formula thatsuccinctlycaptures the reason of infeasibility of paths at the programpoint.  In other words it succinctly captures the reason whypaths through the program point are not buggy.  As a re-sult,  if  the  program  point  is  encountered  again  through  adifferent path such that the interpolant is implied, the newpath can besubsumed, because it can be guaranteed to notbe buggy. The exponential savings are due to the fact that not only is the new path subsumed, but also the paths thatthis new path would spawn by negating its branches.

Unfortunately, methods such as [12, 14, 11] cannot be useddirectly for concolic testing due to several challenges.  First,the  soundness  of  these  methods  relies  on  the  assumptionthat an interpolant at a node has been computed after ex-ploring the entire “tree” of paths that arise from the node.In concolic testing, this assumption is invalid as the testercan impose an arbitrary search order.  For example, concolictesters such as Crest [3] and KLEE [4] use often many heuris-tics that may follow a random walk through the search space,thus making this method unsound.  To address this problem,we need to keep track of nodes whose trees have been ex-plored fully (in which case we say the node is annotated withafull-interpolant) or partially (similarly, ahalf-interpolant).Under this new setting, only nodes with full-interpolants arecapable of subsumption in a sound manner.  As a result, theamount of subsumption depends on how often nodes get an-notated with full-interpolants from the paths explored by theconcolic tester.  Unfortunately our benchmarks in Section 6showed that the above method by itself results in very fewnodes  with  full-interpolants,  thereby  providing  poor  bene-fit to the concolic tester, because the tester rarely exploresthe entire tree of paths arising from a node.  Hence, an im-portant  challenge  now  is  to “accelerate” the  formation  offull-interpolants in order to increase subsumption.  For this,we  introduce  a  novel  technique  calledgreedy  confirmationthat performs limited path exploration (i.e., execution of afew extra paths) by itself, guided by subsumption, with anaim to produce a full-interpolant at nodes currently anno-tated with a half-interpolant.  It is worth mentioning thatthis execution of few paths is done without interfering withthe search order of the concolic tester.  This technique ul-timately  resulted  in  a  significant  increase  in  subsumptionfor our benchmarks, and is vital for the effectiveness of ourmethod.We implemented our method and compared it with a pub-licly available concolic tester, Crest [3].  We found that forthe price of a reasonable overhead to compute interpolants,a large percentage of paths executed by those heuristics canbe subsumed thereby increasing their coverage substantially.

\subsubsection{Result:}
We  attacked  the  path-explosion  problem  of  concolic  test-ing  by  pruning  redundant  paths  using  interpolation.   Thechallenge for interpolation in concolic testing is the lack ofcontrol of search order.  To solve this, we presented the con-cept of half and full interpolants that makes the use of in-terpolants sound, and greedy confirmation that acceleratesthe formation of full-interpolants thereby increasing the like-lihood  of  subsuming  paths. 

% ------------------------------
\subsection{2014: A Context-Guided Search Strategy inConcolic Testing}
\subsubsection{Gist:}
While moststrategies  focus  on  coverage  information  in  the  branch  se-lection process, we introduce CGS which considers contextinformation, that is, how the execution reaches the branch.Our  evaluation  results  show  that  CGS  outperforms  otherstrategies. 

\subsubsection{Methodlogy:}
CGS explores branches in the current execution tree.  Foreach visited branch, CGSexaminesthe branch and decideswhether toselectthe branch for the next input orskipit. CGS looks athow the execution reaches the current branch by calculat-ingk-contextof the branch from its preceding branches anddominator information. Then, thek-contextis comparedwith  the  context  of  previously  selected  branches  which  isstored  in  thecontext  cache.   If  thek-contextis  new,  thebranch is selected for the next input.  Otherwise, CGS skipsthe branch.

\subsubsection{Configurability part:}
We evaluate CGS on top of two publicly available concolictesting tools, CREST [13] and CarFastTool [29]


% ------------------------------
\subsection{2018: Automatically Generating Search Heuristics for Concolic Testing}
\subsubsection{Gist:}
developed a parame-terized search heuristic for concolic testing with an optimizationalgorithm to efficiently search for good parameter values. We hopethat our technique can supplant the laborious and less rewardingtask of manually tuning search heuristics of concolic testing.

\subsubsection{Methodlogy:}
this paper presents a new approachthat automatically generates search heuristics for concolic testing.To this end, we use two key ideas. First, we define aparameterizedsearch heuristic, which creates a large class of search heuristics.The parameterized heuristic reduces the problem of designing agood search heuristic into a problem of finding a good parametervalue. Second, we present a search algorithm specialized to concolictesting. The search space that the parameterized heuristic poses isintractably large. Our algorithm effectively guides the search byiteratively refining the search space based on the feedback fromprevious runs of concolic testing

\subsubsection{Configurability part:}
We have implemented our techniquein CREST [3] and evaluated it on 10 C programs (0.5–150KLoC)

% ------------------------------
\subsection{2018: Template-Guided Concolic Testing via Online Learning}
\subsubsection{Gist:}
a template is a partially symbolized inputvector whose job is to reduce the search space. However, choos-ing a right set of templates is nontrivial and significantly affectsthe final performance of our approach. We present an algorithmthat automatically learns useful templates online, based on datacollected from previous runs of concolic testing. The experimen-tal results with open-source programs show that our techniqueachieves greater branch coverage and finds bugs more effectivelythan conventional concolic testing

In our approach, concolictesting uses a set of templates to exploit common input patterns that improve coverage effectively, where the templates are automat-ically generated through online learning algorithm based on thefeedback from past runs of concolic testing.

\subsubsection{Methodlogy:}
we present template-guided concolic testing, a newtechnique for adaptively reducing the search space of concolic test-ing. The key idea is to guide concolic testing with templates, whichrestrict the input space by selectively generating symbolic variables.Unlike conventional concolic testing that tracks all input valuessymbolically, our technique treats a set of selected input valuesas symbolic and fixes unselected inputs with particular concreteinputs, thereby reducing the original search space. A challenge,however, is choosing input values to track symbolically and replac-ing the remaining inputs with appropriate values. To address thischallenge, we develop an algorithm that performs concolic testingwhile automatically generating, using, and refining templates. Thealgorithm is based on two key ideas. First, by using the sequentialpattern mining [9], we generate the candidate templates from a setof effective test-cases, where the test-cases contribute to improvingcode coverage and are collected while conventional concolic test-ing is performed. Second, we use an algorithm that learns effectivetemplates from the candidates during concolic testing. Our algo-rithm iteratively ranks the candidates based on the effectivenessof templates that were evaluated in the previous runs. Our tech-nique is orthogonal to the existing techniques and can be fruitfullycombined with them, in particular with the state-of-the-art searchheuristics

\subsubsection{Configurability part:}
Experimental results show that our approach outperforms con-ventional concolic testing in term of branch coverage and bug-finding. We have implemented our approach in CREST [7] andcompared our technique with conventional concolic testing foropen-source C programs of medium size (up to 165K LOC). For allbenchmarks, our technique achieves significantly higher branchcoverage compared to conventional concolic testing. For example,for vim-5.7, we have performed both techniques for 70 hours, whereour technique exclusively covered 883 branches that conventionalconcolic testing failed to reach. Our technique also succeeded infinding real bugs that can be triggered in the latest versions of threeopen-source C programs: sed-4.4, grep-3.1 and gawk-4.21.


% ------------------------------
\subsection{2018: Towards Optimal Concolic Testing}
\subsubsection{Gist:}
show the optimal strategy can be defined based onthe probability of program paths and the cost of constraintsolving. The problem of identifying the optimal strategy isthen reduced to a model checking problem of Markov DecisionProcesses with Costs. Secondly, in view of the complexity inidentifying the optimal strategy, we design a greedy algorithmfor approximating the optimal strategy.

\subsubsection{Methodlogy:}
aim to develop a framework which allowsus to define and compute the optimal concolic testing strate-gy. That is, we aim to systematically answer when to applyconcrete execution, when to apply symbolic execution andwhich program path to apply symbolic execution to. In par-ticular, we make the following technical contributions. Firstly,we show that the optimal concolic testing strategy can bedefined based on a probabilistic abstraction of program be-haviors. Secondly, we show that the problem of identifyingthe optimal strategy can be reduced to a model checkingproblem of Markov Decision Processes with Costs. As a re-sult, we can reuse existing tools and algorithms to solve theproblem. Thirdly, we evaluate existing heuristics empiricallyusing a set of simulated experiments and show that theyhave much room to improve. Fourthly, in view of the highcomplexity in computing the optimal strategy, we propose agreedy algorithm which approximates the optimal one. Weempirically evaluate the greedy algorithm based on both sim-ulated experiments and experiments with C programs, andshow that it gains better performance than existing heuristicsin KLEE 

% ------------------------------
\subsection{2018: Qsym : A Practical Concolic Execution Engine Tailored for Hybrid Fuzzing}
\subsubsection{Gist:}
we design a fast concolicexecution engine, calledQSYM, to support hybrid fuzzing.The key idea is to tightly integrate the symbolic emulationwith the native execution using dynamic binary transla-tion, making it possible to implement more fine-grained,so faster, instruction-level symbolic emulation. Addition-ally,QSYMloosens the strict soundness requirements ofconventional concolic executors for better performance,yet takes advantage of a faster fuzzer for validation, pro-viding unprecedented opportunities for performance op-timizations, e.g., optimistically solving constraints andpruning uninteresting basic blocks

\subsubsection{Methodlogy:}
•Fast concolic execution through efficient emula-tion:
We  improved  the  performance  of  concolicexecution by optimizing emulation speed and reduc-ing emulation usage.  Our analysis identified thatsymbol generation emulation was the major perfor-mance bottleneck of concolic execution such that weresolved it with instruction-level selective symbolicexecution, advanced constraints optimization tech-niques, and tied symbolic and concolic executions.

•Efficient repetitive testing and concrete environ-ment.The efficiency ofQSYMmakes re-execution-based repetitive testing and the concrete executionof external environments practical. Because of this,QSYMis free from snapshots incurring significantperformance degradation and incomplete environ-ment models resulting in incorrect symbolic execu-tion due to its non-reusable nature.

•New heuristics for hybrid fuzzing.We proposednew heuristics tailored for hybrid fuzzing to solveunsatisfiable paths optimistically and to prune outcompute-intensive  back  blocks,  thereby  makingQSYMproceed.

\subsubsection{Configurability part:}
Our evaluation shows thatQSYMdoes not just out-perform state-of-the-art fuzzers (i.e.,  found 14×morebugs than VUzzer in the LAVA-M dataset, and outper-formed Driller in104binaries out of126), but also found13 previously unknown security bugsineightreal-worldprograms like Dropbox Lepton, ffmpeg, and OpenJPEG,which have already been intensively tested by the state-of-the-art fuzzers, AFL and OSS-Fuzz.


% ------------------------------
\subsection{2019: Concolic testing with adaptively changing search heuristics}
\subsubsection{Gist:}
adapting search heuristics on the fly via an algorithm that learns new search heuristics based on the knowledge accumulated during concolic testing

\subsubsection{Methodlogy:}
we present an algorithm that automaticallylearns and switches search heuristics during concolic testing. Thealgorithm maintains a set of search heuristics and continuouslychanges them during the testing process. To do so, we first definethe space of possible search heuristics using the idea of parametricsearch heuristic recently proposed in prior work [5]. A technicalchallenge is how to adaptively switch search heuristics in the pre-defined space. We address this challenge with a new concolic testingalgorithm that (1) accumulates the knowledge about the previouslyevaluated search heuristics, (2) learns the probabilistic distributionsof the effective and ineffective search heuristics from the accumu-lated knowledge, and (3) samples a new set of search heuristics from the distributions. The algorithm iteratively performs thesethree steps until it exhausts a given time budget.

% ------------------------------
\subsection{2022: Dr.PathFinder: hybrid fuzzing with deep reinforcement concolic
execution toward deeper path-first search}

\subsubsection{Gist:}
propose a concolic execution algorithm that combines deep reinforcement learning with a hybrid fuzzing
solution, Dr.PathFinder. When the reinforcement learning agent encounters a branch during concolic execution, it evaluates the state and determines the search path. In this process,‘‘shallow’’ paths are pruned, and ‘‘deep’’ paths are searched
first. This reduces unnecessary exploration, allowing the efficient memory usage and alleviating the state explosion
problem.

\subsubsection{Methodlogy:}
We formally define a learning algorithm for a deep
reinforcement learning agent that allows concolic
execution to first search for a deeper path.

We present a deeper path-first search concolic execution algorithm using a reinforcement learning agent and
a hybrid fuzzer called Dr.PathFinder.

\subsubsection{Result:}
In experiments with the CB-multios dataset for deep bug cases, Dr.PathFinder discovered approximately five
times more bugs than AFL and two times more than Driller-AFL. In addition to finding more bugs, Dr.PathFinder
generated 19 times fewer test cases and used at least 2% less memory than Driller-AFL. While it performed well in finding
bugs located in deep paths, Dr.PathFinder had limitation to find bugs located at shallow paths, which we discussed.

% ------------------------------
\subsection{<<paper>>}
\subsubsection{Gist:}

\subsubsection{Methodlogy:}

\subsubsection{Configurability part:}

\subsubsection{Result:}



% ------------------------------
\section{Different catagorical bodies}
see ralated work section of https://dl.acm.org/doi/pdf/10.1145/2635868.2635872
i.e A Context-Guided Search Strategy inConcolic Testing paper

also of https://dl.acm.org/doi/pdf/10.1145/3180155.3180166
i.e 2018: Automatically Generating Search Heuristics for Concolic Testing paper

\begin{itemize}
	\item summerize, give an overview of the main points of each source and combine them into a coherent whole
	\item Analyze and interpret: don’t just paraphrase other researchers—add your own interpretations where possible, discussing the significance of findings in relation to the literature as a whole
	\item Critically evaluate: mention the strengths and weaknesses of your sources
	\item Write in well-structured paragraphs: use transition words and topic sentences to draw connections, comparisons and contrasts
\end{itemize}

\section{Conclusion}

\section{Papers}


\begin{itemize}
  \item 2006: they worked on backtracking algorithms for search heuristics \cite{yan2006backtracking}
  \item 2007: they combined the fuzzing techniques to improve the coverage \cite{majumdar2007hybrid} \cite{godefroid2007compositional} \cite{godefroid2012sage}
  \item 2008: Heuristic based approach to select the branches \cite{kousik2008heuristic}
  \item 2009: they worked on the fitness guided approach to improve the coverage \cite{xie2009fitness}
  \item 2013: they boosted concolic testing by subsuming paths that are guaranteed to not hit a bug with their interpolation technique \cite{jaffar2013boosting}
  \item 2014: they introduced a concept of context guided search strategy \cite{seo2014we}
  \item 2018: automatic selection of suitable heuristic \cite{cha2018automatically}
  \item 2018: template guided approach \cite{cha2018template}
  \item 2018: based on probability of program paths and the cost of constraint solving \cite{wang2018towards}
  \item 2018: they improved the speed of SMT solver by removing the IR layer making it more practical to keep bigger constraints \cite{yun2018qsym}
  \item 2019: fuzzy search strategy \cite{fsct2019}
  \item 2019: adaptably changing search heuristics \cite{adapt2019heuristic}
  \item 2021: Pathcrawler: proposed different strategies to improve the performance of concolic execution on exhaustive branch coverage \cite{pathcrawler2021}
  \item 2022: Dr. Pathfinder \cite{drPathfinder2022} combined concolic execution with deep reinforement learning to prioritize deep paths over shallow ones for hybrid fuzzing
\end{itemize}


%*********************************************************************%
% APPENDIX                                                            %
%*********************************************************************%

% \appendix
% \section{Appendix}
% Insert the appendix here. You can alternatively include files via: \include{pathToFile}

%*********************************************************************%
% LITERATURE                                                          %
%*********************************************************************%
% As a recommendation JabRef might be a useful tool for this section. Use myRefs.bib therefore
\phantomsection
\bibliographystyle{splncs03}
\bibliography{literature}
	
\end{document}
